---
author: "Maks Nikiforov and Mark Austin"
date: "Due 10/31/2021"
params: 
      channel: ""
---


```{r image setup, include=FALSE}

##For markdown automation need a different image folder 
##  for each of the 6 channels
##Also setting up currentChannel variable 
if (params$channel=="data_channel_is_bus") {
  knitr::opts_chunk$set(fig.path = "images/bus/")
  currentChannel<-"Business"
} else if (params$channel=="data_channel_is_entertainment") {
  knitr::opts_chunk$set(fig.path = "images/entertainment/")
  currentChannel<-"Entertainment"
} else if (params$channel=="data_channel_is_lifestyle") {
  knitr::opts_chunk$set(fig.path = "images/lifestyle/")
  currentChannel<-"Lifestyle"
} else if (params$channel=="data_channel_is_socmed") {
  knitr::opts_chunk$set(fig.path = "images/socmed/")
  currentChannel<-"Social Media"
} else if (params$channel=="data_channel_is_tech") {
  knitr::opts_chunk$set(fig.path = "images/tech/")
  currentChannel<-"Tech"
} else if (params$channel=="data_channel_is_world") {
  knitr::opts_chunk$set(fig.path = "images/world/")
  currentChannel<-"World"
} 
#knitr::opts_chunk$set(fig.path = "images/")

```

---
title: "Analysis for the `r currentChannel` Channel"
---



```{r load packages,message=FALSE,echo=FALSE}
   library(tidyverse)
   library(caret)
   library(knitr)
```

## Introduction  


## Import and Prepare Data  

We begin by reading all data into a tibble using `readcsv`.  

```{r import data, message=FALSE}


fullData<-read_csv("./data/OnlineNewsPopularity.csv")


```


The [data documentation](https://archive.ics.uci.edu/ml/datasets/Online+News+Popularity) says variables url and timedelta are nonpredictive so these can be removed.  

```{r remove unused variables}

reduceVarsData<-fullData %>% select(-url,-timedelta)

#Are there other vars we do not need to use??

```



```{r subset for channel}

#test code to be removed later
params$channel<-"data_channel_is_bus"

#filter by the current params channel
channelData<-reduceVarsData %>% filter(eval(as.name(params$channel))==1) 

###Can now drop the data channel variables 
channelData<-channelData %>% select(-starts_with("data_channel"))

# # Dichotomize popularity of articles based on median number of shares
# channelData %>% mutate(popular = NA)
# 
# for (i in 1:length(channelData)) {
#   if (channelData$shares[i] >= median(channelData$shares)) {
#     channelData$popular[i] = 1
#   }
#     else {
#       channelData$popular[i] = 0
#     }
#   }

```


## Summarizations  


### Numerical Summaries  

Summary information for shares.  This gives an idea of the center and spread for shares.  

```{r shares summary}
channelData %>% 
  summarise(Avg = mean(shares), Sd = sd(shares), 
    Median = median(shares), IQR =IQR(shares)) %>% kable(caption = "Summary Statistics for Shares")
```


### Contingency Tables  


```{r shares by size}
##I want to try and split up shares then create a contingency table


```


### Plots  

```{r graphOneA}
##For now this is just a test graph to test out automation
g<-ggplot(data = channelData,
          aes(x=rate_positive_words,y=shares))
g + geom_point() +
  scale_y_continuous(labels = scales::comma) 

```


```{r histogram of shares}

###I'm still working no this plot

###creating histogram of shares data 
g <- ggplot(channelData, aes( x = shares))
g + geom_histogram(binwidth=12000,color = "brown", fill = "green", 
  size = 1)  + labs(x="Shares", y="Count",
  title = "Histogram Shares Counts") +
  scale_y_continuous
  
  scale_x_continuous(labels = scales::comma) 

```



```{r barplot, echo=TRUE, eval=TRUE}
## Bar plot placeholder

# Subset columns to include only weekday_is_*
weekdayData <- channelData %>% select(starts_with("weekday_is"))

# Calculate sum of articles published in each week day
articlesPublished <- lapply(weekdayData, function(c) sum(c=="1"))

weekdayData
articlesPublished

# Use factor to set specific order in bar plot
df <- data.frame(weekday=c("Monday", "Tuesday", "Wednesday", 
                           "Thursday", "Friday", "Saturday", "Sunday"),
                count=articlesPublished)
df$weekday = factor(df$weekday, levels = c("Sunday", "Monday", "Tuesday", "Wednesday", 
                           "Thursday", "Friday", "Saturday"))

# Create bar plot with total publications by day
weekdayBar <- ggplot(df, aes(x = weekday, y = articlesPublished)) + geom_bar(stat = "identity", color = "#123456", fill = "#0072B2") 
weekdayBar + labs(x = "Day", y = "Number published",
       title = "Article publications by day of week") 
```

## Modeling  

```{r splitting data}

#Using set.seed per suggestion so that work will be reproducible
set.seed(20)

channelData$popular <- as.factor(channelData$popular)

dataIndex <-createDataPartition(channelData$shares, p = 0.7, list = FALSE)

channelTrain <-channelData[dataIndex,]
channelTest <-channelData[-dataIndex,]

```


```{r train linear regression model one}

```


```{r train linear regression model two}

```


```{r train random forest model, include=FALSE}

##without parallel code this was still running after 30 minutes so tried parallel next

##my pc has 8 cores so chose 5

##Followed Parallel instructions on caret page
##   https://topepo.github.io/caret/parallel-processing.html
##Even then it took 10 minutest to run
## and picked m=1 so not sure this is working correctly yet?

library(doParallel)
cl <- makePSOCKcluster(5)
registerDoParallel(cl)

rfFit <- train(shares ~ ., data = channelData,
               method = "rf",
               trControl = trainControl(method = "cv",
                                number = 5),
               tuneGrid = data.frame(mtry = 1:15))

stopCluster(cl)

rfFit

```


```{r train boosted tree model, include=FALSE}
# # Create tuning parameters
# nTrees <- c(10, 20, 50, 100, 200, 400)
# interactionDepth = 1:4
# neighbors = c(1, 3, 5, 10, 20)
# 
# set.seed(20)
# # Boosted tree fit
# boostedTreeFit <- train(popular ~ ., data = channelTrain,
#                method = "gbm",
#                preProcess = c("center", "scale"),
#                trControl = trainControl(method = "cv", 
#                                         number = 5),  
#                tuneGrid = expand.grid(n.trees = 10, interaction.depth = 4,
#                                       shrinkage = 0.1, n.minobsinnode = 20))
# 
# #plot(boostedTreeFit)
# #boostedTreeFit$bestTune
# boostedTreeFit
# confusionMatrix(data = channelTest$popular, reference = predict(boostedTreeFit, newdata = channelTest))
```


## Model Comparisions  

This part needs to be automated.  Maybe create a function and iterate over these if they are similar?

```{r test linear regression model one}

```


```{r test linear regression model two}

```


```{r test random forest model}

```


```{r test boosted tree model}

```





